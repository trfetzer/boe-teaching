{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Embedding Illustration with mxbai-embed-large\n\n**Purpose**: Obtain embeddings for a few words and illustrate vector arithmetic.\n\n**Dependencies**: `ollama`, `numpy`, `matplotlib`"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport ollama"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "words = ['man', 'woman', 'king', 'queen', 'prince', 'princess']"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "def embed(text):\n    result = ollama.embeddings(model='mxbai-embed-large', prompt=text)\n    return np.array(result['embedding'])"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "vecs = np.vstack([embed(w) for w in words])\n\n# vector arithmetic: king - man + woman\nking = vecs[words.index('king')]\nman = vecs[words.index('man')]\nwoman = vecs[words.index('woman')]\ntarget = king - man + woman\n\ndef cosine(u, v):\n    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n\nsims = [(w, cosine(target, v)) for w, v in zip(words, vecs)]\nsims.sort(key=lambda x: x[1], reverse=True)\nfor w, s in sims:\n    print(f\"{w:8} {s:.3f}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "proj = vecs[:, :2]\nfor w, p in zip(words, proj):\n    plt.scatter(p[0], p[1])\n    plt.text(p[0] + 0.02, p[1] + 0.02, w)\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}