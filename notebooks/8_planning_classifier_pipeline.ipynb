{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Planning Classifier Pipeline\n\n**Purpose**: Generate synthetic planning descriptions, label a subset with a Pydantic schema, embed them and train classifiers.\n\n**Dependencies**: `pandas`, `numpy`, `scikit-learn`, `torch`, `pydantic`, `joblib`"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "import random\nfrom typing import Literal\n\nimport numpy as np\nimport pandas as pd\n\nfrom pydantic import BaseModel\n\ntry:\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.preprocessing import LabelEncoder\nexcept Exception:\n    LogisticRegression = None\n    LabelEncoder = None\n\ntry:\n    import torch\n    import torch.nn as nn\nexcept Exception:\n    torch = None\n    nn = None\n\ntry:\n    import joblib\nexcept Exception:\n    joblib = None\n\nsimulate = True\nEMBED_DIM = 256"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class PlanningClassification(BaseModel):\n    main_type: Literal['new_home_construction','modification_or_extension','change_of_use','demolition','conditions_or_amendments','tree_works','advertisement_or_signage','unknown']\n    sector: Literal['residential','retail','industrial','agricultural','educational','renewable_energy','infrastructure','hospitality_or_leisure','office','unknown']\n\ndef generate_documents(n=30):\n    templates = [\n        'Erection of 2 new houses with garages',\n        'Change of use from warehouse to office',\n        'Demolition of existing barn and building 3 flats',\n        'Installation of rooftop solar panels',\n        'Removal of oak tree protected by TPO',\n    ]\n    docs = [random.choice(templates)+f' #{i}' for i in range(n)]\n    return pd.DataFrame({'text': docs, 'id': range(n)})\n\ndef embed(text):\n    if simulate or torch is None:\n        return np.random.rand(EMBED_DIM).astype(np.float32)\n    from ollama import Client\n    client = Client(host='http://localhost:11434')\n    r = client.embeddings(model='mxbai-embed-large', prompt=text)\n    return np.array(r['embedding'], dtype=np.float32)\n\ndef label_text(text):\n    if simulate:\n        main = random.choice(list(PlanningClassification.model_fields['main_type'].annotation.__args__))\n        sec = random.choice(list(PlanningClassification.model_fields['sector'].annotation.__args__))\n        return PlanningClassification(main_type=main, sector=sec)\n    from ollama import Client\n    client = Client(host='http://localhost:11434')\n    prompt = f'You are an expert in UK planning law. Classify this planning application:\n{text}\nReturn compact JSON as {{\"main_type\": \"...\", \"sector\": \"...\"}}'\n    r = client.chat(model='llama3.2:3b-instruct-fp16', messages=[{'role':'user','content':prompt}])\n    return PlanningClassification.model_validate_json(r['message']['content'].strip())"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "docs = generate_documents(30)\ndocs['embedding'] = docs['text'].apply(embed)\n\nlabelled = docs.sample(n=10, random_state=42).copy()\nlabels = [label_text(t) for t in labelled['text']]\nlabelled['main_type'] = [l.main_type for l in labels]\nlabelled['sector'] = [l.sector for l in labels]\n\nX = np.vstack(labelled['embedding'])\n\nif LogisticRegression is None or LabelEncoder is None:\n    print('scikit-learn not installed; skipping training.')\nelse:\n    le_main = LabelEncoder().fit(labelled['main_type'])\n    y_main = le_main.transform(labelled['main_type'])\n    clf_main = LogisticRegression(max_iter=200).fit(X, y_main)\n    if joblib:\n        joblib.dump(clf_main, 'main_type_logreg.joblib')\n        joblib.dump(le_main, 'main_type_encoder.joblib')\n\nif torch and nn:\n    le_sector = LabelEncoder().fit(labelled['sector'])\n    y_sec = torch.tensor(le_sector.transform(labelled['sector']), dtype=torch.long)\n    X_tensor = torch.tensor(X, dtype=torch.float32)\n    model = nn.Sequential(nn.Linear(EMBED_DIM,64), nn.ReLU(), nn.Linear(64, len(le_sector.classes_)))\n    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n    loss_fn = nn.CrossEntropyLoss()\n    for _ in range(50):\n        opt.zero_grad(); out = model(X_tensor); loss = loss_fn(out, y_sec); loss.backward(); opt.step()\n    torch.save(model.state_dict(), 'sector_mlp.pt')\n    if joblib: joblib.dump(le_sector, 'sector_encoder.joblib')\nelse:\n    print('PyTorch not installed; skipped MLP training.')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
